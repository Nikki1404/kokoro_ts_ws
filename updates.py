import asyncio
import json
import time
import wave
import io
from pathlib import Path
from typing import Tuple
import boto3
import websockets
import jiwer
import pandas as pd
from tqdm import tqdm
from whisper_normalizer.english import EnglishTextNormalizer
import util


URL = "ws://127.0.0.1:8000/ws"
BUCKET = "cx-speech"
PREFIX = "asr-realtime/benchmarking-data-3/"

TARGET_SR = 16000
CHUNK_MS = 80
CHUNK_FRAMES = int(TARGET_SR * CHUNK_MS / 1000)

s3 = boto3.client("s3")

whisper_norm = EnglishTextNormalizer()


def normalize(txt):
    return whisper_norm(txt or "")


def list_s3_subfolders():
    paginator = s3.get_paginator("list_objects_v2")
    result = paginator.paginate(
        Bucket=BUCKET,
        Prefix=PREFIX,
        Delimiter="/"
    )

    folders = []
    for page in result:
        for prefix in page.get("CommonPrefixes", []):
            folders.append(prefix["Prefix"])
    return folders


def get_s3_object_bytes(key: str) -> bytes:
    obj = s3.get_object(Bucket=BUCKET, Key=key)
    return obj["Body"].read()


def iter_wav_chunks_from_bytes(wav_bytes: bytes):
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        while True:
            data = wf.readframes(CHUNK_FRAMES)
            if not data:
                break
            yield data


def silence_bytes(sec: float) -> bytes:
    return b"\x00\x00" * int(TARGET_SR * sec)


async def transcribe_ws(wav_bytes: bytes) -> Tuple[str, float, int, int]:

    async with websockets.connect(URL, max_size=None) as ws:

        finals = []
        final_received = asyncio.Event()

        first_partial_time = None
        t_start = None

        async def receiver():
            nonlocal first_partial_time
            async for msg in ws:
                if isinstance(msg, str):
                    obj = json.loads(msg)

                    # TTFT (first partial)
                    if obj.get("type") == "partial":
                        if obj.get("text") and first_partial_time is None:
                            first_partial_time = time.time()

                    # Final
                    if obj.get("type") == "final":
                        if obj.get("text"):
                            finals.append(obj["text"].strip())
                        final_received.set()

        recv_task = asyncio.create_task(receiver())

        frames_sent = 0
        t_start = time.time()

        for chunk in iter_wav_chunks_from_bytes(wav_bytes):
            await ws.send(chunk)
            frames_sent += len(chunk) // 2
            await asyncio.sleep(CHUNK_MS / 1000.0)  # simulate realtime streaming

        # trigger finalization via silence
        await ws.send(silence_bytes(0.7))

        await asyncio.wait_for(final_received.wait(), timeout=60)

        t_end = time.time()

        # TTFT
        if first_partial_time:
            ttft_ms = int((first_partial_time - t_start) * 1000)
        else:
            ttft_ms = None

        # latency_ms (TTF)
        latency_ms = int((t_end - t_start) * 1000)

        audio_sec_sent = frames_sent / TARGET_SR

        await ws.close()
        recv_task.cancel()

        return " ".join(finals), audio_sec_sent, latency_ms, ttft_ms


transform = jiwer.Compose([
    jiwer.ToLowerCase(),
    jiwer.RemovePunctuation(),
    jiwer.RemoveMultipleSpaces(),
    jiwer.Strip(),
    jiwer.ReduceToListOfListOfWords(word_delimiter=" "),
])


def calculate_wer(reference_str: str, hypothesis_str: str) -> float:
    return jiwer.wer(
        reference=reference_str,
        hypothesis=hypothesis_str,
        reference_transform=transform,
        hypothesis_transform=transform
    )


async def process_folder(folder_prefix: str):
    objects = s3.list_objects_v2(Bucket=BUCKET, Prefix=folder_prefix)["Contents"]

    wav_key = None
    txt_key = None

    for obj in objects:
        if obj["Key"].endswith(".wav"):
            wav_key = obj["Key"]
        if obj["Key"].endswith(".txt"):
            txt_key = obj["Key"]

    if not wav_key or not txt_key:
        return None

    wav_bytes = get_s3_object_bytes(wav_key)
    reference_text = get_s3_object_bytes(txt_key).decode("utf-8")

    hyp, audio_sec, latency_ms, ttft_ms = await transcribe_ws(wav_bytes)

    normalized_ref = normalize(reference_text)
    normalized_hyp = normalize(hyp)

    return {
        util.FILENAME_COLUMN_NAME: folder_prefix.split("/")[-2],

        util.REFERENCE_TEXT_COLUMN_NAME: reference_text,
        util.get_transcript_column_name("nemotron"): hyp,
        util.get_wer_column_name("nemotron"): calculate_wer(reference_text, hyp),

        util.REFERENCE_TEXT_NORMALIZED_COLUMN_NAME: normalized_ref,
        util.get_transcript_normalized_column_name("nemotron"): normalized_hyp,
        util.get_wer_for_transcript_and_reference_normalized_column_name("nemotron"):
            calculate_wer(normalized_ref, normalized_hyp),

        "latency_ms": latency_ms,
        "ttft_ms": ttft_ms,
        "audio_sec": round(audio_sec, 2),
    }


def write_to_file(results):
    df = pd.DataFrame(results)
    df.to_excel("benchmark_results_asr_realtime.xlsx", index=False)


async def main():
    folders = list_s3_subfolders()
    results = []

    for folder in tqdm(folders):
        try:
            result = await process_folder(folder)
            if result:
                results.append(result)
        except Exception as e:
            print("Error in folder:", folder, e)

    write_to_file(results)


if __name__ == "__main__":
    asyncio.run(main())
